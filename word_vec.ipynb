{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word_vec.ipynb",
      "provenance": [],
      "mount_file_id": "1-BNN646q9vAG-C6anpeQkMdJCSa16HlL",
      "authorship_tag": "ABX9TyMpfEndThavqmN3npfUolwZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sameepshrestha/Mission-Chatbot/blob/main/word_vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqtLS1LTnqoo"
      },
      "source": [
        "word2vec model : https://arxiv.org/pdf/1310.4546.pdf \n",
        "\n",
        "used for representing the different words as  vector capturing the context on which they are used.\n",
        "First things first let us get the total vocab of our data ,we still need a little cleaning left in our data lets do that first shall we."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QM7APp65S6Hh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eeae631e-bcff-47e4-e4d3-cc6b24921da6"
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras \n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization \n",
        "import tqdm\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dot, Embedding, Flatten\n",
        "%matplotlib"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using matplotlib backend: agg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3yzaf0u3OQv"
      },
      "source": [
        "SEED = 42\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l630yOaHq-f7",
        "outputId": "305d797f-13d0-4e66-a431-0bf2da08d299"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "3tYRDAmAq_hm",
        "outputId": "dd935041-a5f0-477c-c931-d1426e0b2cd3"
      },
      "source": [
        "dataset = pd.read_csv('/content/drive/MyDrive/chat bots/final_dataset.csv')\n",
        "dataset.head()"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>From</th>\n",
              "      <th>Message</th>\n",
              "      <th>reply</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Puspa Rai</td>\n",
              "      <td>O ho k x hou</td>\n",
              "      <td>Thik. Xa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Dipesh Chapagain</td>\n",
              "      <td>Hi Science test tomarrow</td>\n",
              "      <td>hlo \\ni am a friend of sameep...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Spponge Bob</td>\n",
              "      <td>hhahaha sale malai k taha :D talai taha theyo ...</td>\n",
              "      <td>Ah ta aja tst ma gad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Spponge Bob</td>\n",
              "      <td>hhahahhaa k test ma pass hunxas ta :D</td>\n",
              "      <td>Xup pas ta humxu ful marks audaina bujis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Spponge Bob</td>\n",
              "      <td>hhahhaha :D :D sale maile tero wall pic hali d...</td>\n",
              "      <td>Ha kasari xange hanu pas</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               From  ...                                     reply\n",
              "0         Puspa Rai  ...                                  Thik. Xa\n",
              "1  Dipesh Chapagain  ...          hlo \\ni am a friend of sameep...\n",
              "2       Spponge Bob  ...                      Ah ta aja tst ma gad\n",
              "3       Spponge Bob  ...  Xup pas ta humxu ful marks audaina bujis\n",
              "4       Spponge Bob  ...                  Ha kasari xange hanu pas\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewWGAfsfrQWv"
      },
      "source": [
        "import re\n",
        "def remove(message):\n",
        "    pure = ''\n",
        "    for letter in message:\n",
        "        if letter in 'abcdefghijklmnopqrstuvwxyz' or letter == ' ':\n",
        "              pure = pure + letter\n",
        "    # print(pure)\n",
        "    return pure\n",
        "        \n",
        "def remove_haha(message):\n",
        "    sentence=''\n",
        "    for word in message.split():\n",
        "        if re.search('haha',word):\n",
        "            sentence = sentence+' '+ 'haha'\n",
        "        else :\n",
        "            sentence = sentence+' '+ word\n",
        "    return sentence\n",
        "def remove_and_clean(message):  \n",
        "    \n",
        "    clean_message = remove(message.lower())\n",
        "    clean_message = remove_haha(clean_message)\n",
        "    # print(clean_message)\n",
        "    return clean_message \n",
        "dataset['clean_message']= dataset.apply(lambda x : remove_and_clean(x['Message']) ,axis=1)\n",
        "dataset['clean_reply']= dataset.apply(lambda x : remove_and_clean(x['reply']) ,axis=1)"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaNSb5CDWWTp"
      },
      "source": [
        "removing the unnecessary long messages to decrease the sequence of the model to make the padding necessary for sentence with smaller number of words be small so that the model dosent fit on the padding data\n",
        "\n",
        "'from about 38k data 1400 empty and long message dataset were deleted"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAooXYCqx7EE",
        "outputId": "cc32539c-9749-4b86-a447-7a33038539f6"
      },
      "source": [
        "largest = []\n",
        "vocab_size = []\n",
        "vocab = []\n",
        "empty = []\n",
        "for i,(line1, line2) in enumerate(zip(dataset['clean_message'][0:],dataset['clean_reply'][0:])):\n",
        "    len1 = len(line1.split())\n",
        "    len2 = len(line2.split())\n",
        "\n",
        "    vocab_size.append(len1)\n",
        "    vocab_size.append(len2)\n",
        "    if len1 == 0 or len2 == 0 or len1 >=30 or len2>=30 :\n",
        "        empty.append(i)\n",
        "    else:\n",
        "        for word1 in line1.split():\n",
        "            if word1 not in vocab:\n",
        "                vocab.append(word1)\n",
        "        for word2 in line2.split():\n",
        "            if word2 not in vocab:\n",
        "                vocab.append(word2)\n",
        "print(len(vocab))\n",
        "        "
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "38533\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "id": "Ww7Op0EFSPSF",
        "outputId": "edae5e45-a2c9-444d-dabe-2455992c17d7"
      },
      "source": [
        "len(empty)\n",
        "dataset = dataset.drop(labels=None,axis=0,index=empty)\n",
        "dataset.describe()"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>From</th>\n",
              "      <th>Message</th>\n",
              "      <th>reply</th>\n",
              "      <th>clean_message</th>\n",
              "      <th>clean_reply</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>36152</td>\n",
              "      <td>36152</td>\n",
              "      <td>36152</td>\n",
              "      <td>36152</td>\n",
              "      <td>36152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>297</td>\n",
              "      <td>33708</td>\n",
              "      <td>33442</td>\n",
              "      <td>32919</td>\n",
              "      <td>32831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>Narmada Dahal</td>\n",
              "      <td>Ah</td>\n",
              "      <td>ah</td>\n",
              "      <td>haha</td>\n",
              "      <td>ah</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>7318</td>\n",
              "      <td>74</td>\n",
              "      <td>200</td>\n",
              "      <td>153</td>\n",
              "      <td>260</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 From Message  reply clean_message clean_reply\n",
              "count           36152   36152  36152         36152       36152\n",
              "unique            297   33708  33442         32919       32831\n",
              "top     Narmada Dahal      Ah     ah          haha          ah\n",
              "freq             7318      74    200           153         260"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "id": "6z2QLoxaTSrm",
        "outputId": "3e7f0058-3492-41ef-90f2-165d63df3777"
      },
      "source": [
        "# dataset.describe()\n",
        "dataset.head()"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>From</th>\n",
              "      <th>Message</th>\n",
              "      <th>reply</th>\n",
              "      <th>clean_message</th>\n",
              "      <th>clean_reply</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Puspa Rai</td>\n",
              "      <td>O ho k x hou</td>\n",
              "      <td>Thik. Xa</td>\n",
              "      <td>o ho k x hou</td>\n",
              "      <td>thik xa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Dipesh Chapagain</td>\n",
              "      <td>Hi Science test tomarrow</td>\n",
              "      <td>hlo \\ni am a friend of sameep...</td>\n",
              "      <td>hi science test tomarrow</td>\n",
              "      <td>hlo i am a friend of sameep</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Spponge Bob</td>\n",
              "      <td>hhahaha sale malai k taha :D talai taha theyo ...</td>\n",
              "      <td>Ah ta aja tst ma gad</td>\n",
              "      <td>haha sale malai k taha d talai taha theyo jas...</td>\n",
              "      <td>ah ta aja tst ma gad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Spponge Bob</td>\n",
              "      <td>hhahahhaa k test ma pass hunxas ta :D</td>\n",
              "      <td>Xup pas ta humxu ful marks audaina bujis</td>\n",
              "      <td>haha k test ma pass hunxas ta d</td>\n",
              "      <td>xup pas ta humxu ful marks audaina bujis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Spponge Bob</td>\n",
              "      <td>hhahhaha :D :D sale maile tero wall pic hali d...</td>\n",
              "      <td>Ha kasari xange hanu pas</td>\n",
              "      <td>haha d d sale maile tero wall pic hali daeko ...</td>\n",
              "      <td>ha kasari xange hanu pas</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               From  ...                                clean_reply\n",
              "0         Puspa Rai  ...                                    thik xa\n",
              "1  Dipesh Chapagain  ...                hlo i am a friend of sameep\n",
              "2       Spponge Bob  ...                       ah ta aja tst ma gad\n",
              "3       Spponge Bob  ...   xup pas ta humxu ful marks audaina bujis\n",
              "4       Spponge Bob  ...                   ha kasari xange hanu pas\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5gGDoZ8neIv"
      },
      "source": [
        "This part of the code is from the tensorflow itself. Couple of things happening in this as recquired from the paper discussed in the paper is :\n",
        "\n",
        "\n",
        "*   skip grams making for the sentence given\n",
        "*   next is making the context and target \n",
        "*   than for the negative sampling we recquire some words selected in randm to apply sigmoid or binary classification \n",
        "*   softmax coputionally expensive so this is selected as summing up for 40k for every step will be quite extensive\n",
        "\n",
        "A full description can be found in tensorflow dataset \n",
        "\n",
        "https://www.tensorflow.org/tutorials/text/word2vec\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKaNQkIKxWCC"
      },
      "source": [
        "all_messages = pd.concat([dataset['clean_message'],dataset['clean_reply']],axis=0)\n",
        "all_messages = all_messages.reset_index(drop=True)"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J776on04x8mQ"
      },
      "source": [
        "text_dataset = tf.data.Dataset.from_tensor_slices(all_messages)"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeUMXBuzyhPG"
      },
      "source": [
        "#text vectorizer \n",
        "vocab_size = 38535\n",
        "sequence_length = 30\n",
        "vectorize_layer = TextVectorization(\n",
        "                    split= 'whitespace',\n",
        "                    output_mode='int',\n",
        "                    max_tokens=vocab_size,\n",
        "                    output_sequence_length=sequence_length\n",
        "                    )\n",
        "vectorize_layer.adapt(text_dataset.batch(1024))"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4n7rmgkEgiLw",
        "outputId": "ea1e4388-cdf9-4360-ff43-367204e9b6e6"
      },
      "source": [
        "vocab = vectorize_layer.get_vocabulary()\n",
        "vocab[:20]"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " 'ma',\n",
              " 'ho',\n",
              " 'ko',\n",
              " 'ni',\n",
              " 'ta',\n",
              " 'k',\n",
              " 'xa',\n",
              " 'haha',\n",
              " 'la',\n",
              " 'tah',\n",
              " 'tyo',\n",
              " 'ani',\n",
              " 'ae',\n",
              " 'ah',\n",
              " 'hunxa',\n",
              " 'chai',\n",
              " 'nai',\n",
              " 'ra']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-OuGKi40l_S"
      },
      "source": [
        "text_vector_ds = text_dataset.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAJRlbaH1Oea",
        "outputId": "c3bd8ff0-5d3e-41ad-ce2c-4196a89dc9b4"
      },
      "source": [
        "sequences = list(text_vector_ds.as_numpy_iterator())\n",
        "print(len(sequences))\n",
        "for seq in sequences[:5]:\n",
        "  print(f\"{seq} => {[vocab[i] for i in seq]}\")"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "72304\n",
            "[875   3   7 337 898   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0] => ['o', 'ho', 'k', 'x', 'hou', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "[  584   570   430 18455     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0] => ['hi', 'science', 'test', 'tomarrow', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "[    9   229    30     7  1408   202   156  1408  1843   448 27235   148\n",
            "    99   135     3    12   202     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0] => ['haha', 'sale', 'malai', 'k', 'taha', 'd', 'talai', 'taha', 'theyo', 'jastai', 'lagod', 'oe', 'tero', 'sir', 'ho', 'tyo', 'd', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "[   9    7  430    2  348 3143    6  202    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0] => ['haha', 'k', 'test', 'ma', 'pass', 'hunxas', 'ta', 'd', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "[    9   202   202   229    29    99   901   398  4838 33923    88    13\n",
            "   229    99   551  4480    74   397    20   253  5169  1650 22284     0\n",
            "     0     0     0     0     0     0] => ['haha', 'd', 'd', 'sale', 'maile', 'tero', 'wall', 'pic', 'hali', 'daeko', 'xu', 'ani', 'sale', 'tero', 'id', 'hack', 'huna', 'sakxa', 'hai', 'alik', 'laamo', 'password', 'raakh', '', '', '', '', '', '', '']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKAE-sFq3wjQ"
      },
      "source": [
        "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
        "# (int-encoded sentences) based on window size, number of negative samples\n",
        "# and vocabulary size.\n",
        "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
        "  # Elements of each training example are appended to these lists.\n",
        "  targets, contexts, labels = [], [], []\n",
        "\n",
        "  # Build the sampling table for vocab_size tokens.\n",
        "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
        "\n",
        "  # Iterate over all sequences (sentences) in dataset.\n",
        "  for sequence in tqdm.tqdm(sequences):\n",
        "\n",
        "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
        "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "          sequence,\n",
        "          vocabulary_size=vocab_size,\n",
        "          sampling_table=sampling_table,\n",
        "          window_size=window_size,\n",
        "          negative_samples=0)\n",
        "\n",
        "    # Iterate over each positive skip-gram pair to produce training examples\n",
        "    # with positive context word and negative samples.\n",
        "    for target_word, context_word in positive_skip_grams:\n",
        "      context_class = tf.expand_dims(\n",
        "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
        "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "          true_classes=context_class,\n",
        "          num_true=1,\n",
        "          num_sampled=num_ns,\n",
        "          unique=True,\n",
        "          range_max=vocab_size,\n",
        "          seed=SEED,\n",
        "          name=\"negative_sampling\")\n",
        "\n",
        "      # Build context and label vectors (for one target word)\n",
        "      negative_sampling_candidates = tf.expand_dims(\n",
        "          negative_sampling_candidates, 1)\n",
        "\n",
        "      context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
        "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
        "\n",
        "      # Append each element from the training example to global lists.\n",
        "      targets.append(target_word)\n",
        "      contexts.append(context)\n",
        "      labels.append(label)\n",
        "\n",
        "  return targets, contexts, labels"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKdioUEg4brM",
        "outputId": "0fe41eaf-f06d-4b70-890b-412369d06a59"
      },
      "source": [
        "targets, contexts, labels = generate_training_data(\n",
        "    sequences=sequences,\n",
        "    window_size=2,\n",
        "    num_ns=4,\n",
        "    vocab_size=vocab_size,\n",
        "    seed=SEED)\n",
        "print(len(targets), len(contexts), len(labels))"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 72304/72304 [00:47<00:00, 1517.16it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "407184 407184 407184\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvLx0L0n4clg"
      },
      "source": [
        "BATCH_SIZE = 1024\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "num_ns = 4"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJ8epINB7Kri",
        "outputId": "62c1614c-73ec-4a55-b61e-a042c33e5957"
      },
      "source": [
        "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "print(dataset)"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<PrefetchDataset shapes: (((1024,), (1024, 5, 1)), (1024, 5)), types: ((tf.int32, tf.int64), tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBKxFCRd7QcB"
      },
      "source": [
        "class Word2Vec(Model):\n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "    super(Word2Vec, self).__init__()\n",
        "    self.target_embedding = Embedding(vocab_size,\n",
        "                                      embedding_dim,\n",
        "                                      input_length=1,\n",
        "                                      name=\"w2v_embedding\")\n",
        "    self.context_embedding = Embedding(vocab_size,\n",
        "                                       embedding_dim,\n",
        "                                       input_length=num_ns+1)\n",
        "    self.dots = Dot(axes=(3, 2))\n",
        "    self.flatten = Flatten()\n",
        "\n",
        "  def call(self, pair):\n",
        "    target, context = pair\n",
        "    word_emb = self.target_embedding(target)\n",
        "    context_emb = self.context_embedding(context)\n",
        "    dots = self.dots([context_emb, word_emb])\n",
        "    return self.flatten(dots)"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2HiLYyJ7X-R"
      },
      "source": [
        "embedding_dim = 128\n",
        "num_ns = 4\n",
        "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
        "word2vec.compile(optimizer='adam',\n",
        "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "                 metrics=['accuracy'])"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zM1v_JqG7t8I",
        "outputId": "4c9dea9d-491b-4f0c-b5b1-2a99c823d9b4"
      },
      "source": [
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
        "word2vec.fit(dataset, epochs=40, callbacks=[tensorboard_callback])"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "397/397 [==============================] - 45s 111ms/step - loss: 1.6020 - accuracy: 0.2364\n",
            "Epoch 2/40\n",
            "397/397 [==============================] - 41s 104ms/step - loss: 1.5058 - accuracy: 0.4107\n",
            "Epoch 3/40\n",
            "397/397 [==============================] - 41s 105ms/step - loss: 1.3549 - accuracy: 0.5143\n",
            "Epoch 4/40\n",
            "397/397 [==============================] - 41s 104ms/step - loss: 1.1517 - accuracy: 0.6610\n",
            "Epoch 5/40\n",
            "397/397 [==============================] - 41s 103ms/step - loss: 0.9414 - accuracy: 0.7633\n",
            "Epoch 6/40\n",
            "397/397 [==============================] - 41s 104ms/step - loss: 0.7574 - accuracy: 0.8266\n",
            "Epoch 7/40\n",
            "397/397 [==============================] - 42s 106ms/step - loss: 0.6081 - accuracy: 0.8675\n",
            "Epoch 8/40\n",
            "397/397 [==============================] - 41s 103ms/step - loss: 0.4916 - accuracy: 0.8956\n",
            "Epoch 9/40\n",
            "397/397 [==============================] - 41s 104ms/step - loss: 0.4024 - accuracy: 0.9161\n",
            "Epoch 10/40\n",
            "397/397 [==============================] - 42s 106ms/step - loss: 0.3343 - accuracy: 0.9327\n",
            "Epoch 11/40\n",
            "397/397 [==============================] - 41s 103ms/step - loss: 0.2819 - accuracy: 0.9451\n",
            "Epoch 12/40\n",
            "397/397 [==============================] - 43s 108ms/step - loss: 0.2408 - accuracy: 0.9541\n",
            "Epoch 13/40\n",
            "397/397 [==============================] - 41s 103ms/step - loss: 0.2082 - accuracy: 0.9607\n",
            "Epoch 14/40\n",
            "397/397 [==============================] - 42s 105ms/step - loss: 0.1819 - accuracy: 0.9661\n",
            "Epoch 15/40\n",
            "397/397 [==============================] - 40s 102ms/step - loss: 0.1603 - accuracy: 0.9706\n",
            "Epoch 16/40\n",
            "397/397 [==============================] - 41s 104ms/step - loss: 0.1425 - accuracy: 0.9738\n",
            "Epoch 17/40\n",
            "397/397 [==============================] - 42s 107ms/step - loss: 0.1276 - accuracy: 0.9767\n",
            "Epoch 18/40\n",
            "397/397 [==============================] - 42s 105ms/step - loss: 0.1150 - accuracy: 0.9787\n",
            "Epoch 19/40\n",
            "397/397 [==============================] - 41s 104ms/step - loss: 0.1044 - accuracy: 0.9804\n",
            "Epoch 20/40\n",
            "397/397 [==============================] - 40s 101ms/step - loss: 0.0953 - accuracy: 0.9818\n",
            "Epoch 21/40\n",
            "397/397 [==============================] - 41s 103ms/step - loss: 0.0875 - accuracy: 0.9829\n",
            "Epoch 22/40\n",
            "397/397 [==============================] - 41s 104ms/step - loss: 0.0807 - accuracy: 0.9839\n",
            "Epoch 23/40\n",
            "397/397 [==============================] - 41s 104ms/step - loss: 0.0749 - accuracy: 0.9848\n",
            "Epoch 24/40\n",
            "397/397 [==============================] - 40s 100ms/step - loss: 0.0698 - accuracy: 0.9855\n",
            "Epoch 25/40\n",
            "397/397 [==============================] - 42s 105ms/step - loss: 0.0654 - accuracy: 0.9860\n",
            "Epoch 26/40\n",
            "397/397 [==============================] - 40s 101ms/step - loss: 0.0615 - accuracy: 0.9864\n",
            "Epoch 27/40\n",
            "397/397 [==============================] - 42s 105ms/step - loss: 0.0581 - accuracy: 0.9868\n",
            "Epoch 28/40\n",
            "397/397 [==============================] - 42s 105ms/step - loss: 0.0551 - accuracy: 0.9871\n",
            "Epoch 29/40\n",
            "397/397 [==============================] - 42s 106ms/step - loss: 0.0525 - accuracy: 0.9874\n",
            "Epoch 30/40\n",
            "397/397 [==============================] - 42s 105ms/step - loss: 0.0502 - accuracy: 0.9876\n",
            "Epoch 31/40\n",
            "397/397 [==============================] - 42s 105ms/step - loss: 0.0481 - accuracy: 0.9877\n",
            "Epoch 32/40\n",
            "397/397 [==============================] - 41s 104ms/step - loss: 0.0463 - accuracy: 0.9878\n",
            "Epoch 33/40\n",
            "397/397 [==============================] - 46s 116ms/step - loss: 0.0446 - accuracy: 0.9879\n",
            "Epoch 34/40\n",
            "397/397 [==============================] - 47s 119ms/step - loss: 0.0432 - accuracy: 0.9881\n",
            "Epoch 35/40\n",
            "397/397 [==============================] - 41s 104ms/step - loss: 0.0419 - accuracy: 0.9882\n",
            "Epoch 36/40\n",
            "397/397 [==============================] - 46s 117ms/step - loss: 0.0407 - accuracy: 0.9883\n",
            "Epoch 37/40\n",
            "397/397 [==============================] - 52s 131ms/step - loss: 0.0397 - accuracy: 0.9883\n",
            "Epoch 38/40\n",
            "397/397 [==============================] - 44s 110ms/step - loss: 0.0387 - accuracy: 0.9884\n",
            "Epoch 39/40\n",
            "397/397 [==============================] - 46s 117ms/step - loss: 0.0379 - accuracy: 0.9885\n",
            "Epoch 40/40\n",
            "397/397 [==============================] - 55s 138ms/step - loss: 0.0371 - accuracy: 0.9885\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe057c8f0d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xA-jwFmn83fo"
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdWqYZC8BHYV"
      },
      "source": [
        "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
        "vocab = vectorize_layer.get_vocabulary()\n",
        "import io"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rh9_DXaIFp5t"
      },
      "source": [
        "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
        "\n",
        "for index, word in enumerate(vocab):\n",
        "  if index == 0:\n",
        "    continue  # skip 0, it's padding.\n",
        "  vec = weights[index]\n",
        "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
        "  out_m.write(word + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "5OwqVDpyFsHm",
        "outputId": "80a57a29-621a-46a8-febf-f9fded23f268"
      },
      "source": [
        "try:\n",
        "  from google.colab import files\n",
        "  files.download('vectors.tsv')\n",
        "  files.download('metadata.tsv')\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_fe01dcd1-6307-448e-b45c-f6a0bd1330e5\", \"vectors.tsv\", 56619337)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_cf8f8a0f-17c4-4c20-b7c8-d4c1ec04e877\", \"metadata.tsv\", 295257)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EMNEn0tGhBe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}