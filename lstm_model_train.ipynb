{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm_model_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1jdzNaMDwFBd-YZ1d5LBMp4WL2eFKGpkU",
      "authorship_tag": "ABX9TyMylwa4bqIisxDG9F/MlX3w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sameepshrestha/Mission-Chatbot/blob/main/lstm_model_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oK872g-oMSg4"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import LSTM\n",
        "import csv \n",
        "import pandas as pd \n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.activations import  sigmoid, tanh\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np \n",
        "# from sklearn.preprocessing import train_test_split\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RtgWiym7Zhc"
      },
      "source": [
        "# encoder and decoder both input_shape = [batch ,time_step,feature] \n",
        "batch_size = 64\n",
        "time_step = 31\n",
        "epochs = 200\n",
        "vector_shape  = 300  #the loss we wil be using is the cosine \n",
        "#the ct and the ht learned from the encoder as a context is passed to the decoder \n",
        "data_set = pd.read_csv('/content/drive/MyDrive/chat bots/trainable_dataset.csv')\n",
        "count = len(data_set)\n",
        "count=25000"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yNWn1LRJiPH"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9TLVpGr1qDa"
      },
      "source": [
        "\n",
        "vocab = {}\n",
        "vocab_reverse = {}\n",
        "with open ('/content/drive/MyDrive/chat bots/metadata (1).tsv') as f:\n",
        "    for i,row in enumerate(csv.reader(f )):\n",
        "        vocab[row[0]] = i\n",
        "        vocab_reverse[i] = row[0]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTctAe9i1_FB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "9a161242-0bc3-4f22-ec96-3d4c1f0544d4"
      },
      "source": [
        "\n",
        "vectors = np.zeros([total_size,vector_shape])\n",
        "with open ('/content/drive/MyDrive/chat bots/vectors (1).tsv') as f:\n",
        "    for i,row in enumerate(csv.reader(f,delimiter=\"\\t\")):\n",
        "        vectors[i,:] = [np.array(float(x)) for x in row]\n",
        "print(vectors.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-8a17bfb5fe96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtotal_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvector_shape\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/chat bots/vectors (1).tsv'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'total_size' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxZ-RZfa72Nf"
      },
      "source": [
        "# well this is a mess no stacked as one opposes stack and other support it but i think the attention model \n",
        "#without the stack should also be the best option >next target if this becomes some what sucessful\n",
        "sequence_length =  30\n",
        "encoder_inputs = Input(shape=( sequence_length,vector_shape))\n",
        "encoder = LSTM(300, return_sequences=False, return_state=True)\n",
        "encoder_outputs, stateh, statec = encoder(encoder_inputs)\n",
        "encoder_states = [stateh,statec]\n",
        "decoder_inputs = Input(shape= (None,vector_shape))\n",
        "decoder = LSTM(300,return_sequences=True, return_state=True )\n",
        "decoder_outputs,_ ,_  = decoder(decoder_inputs, initial_state = encoder_states)\n",
        "\n",
        "decoder_dense = Dense(vector_shape, activation = tanh)\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "seq2seq_model = Model([encoder_inputs,decoder_inputs],decoder_outputs)\n",
        "seq2seq_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFCeQS2-369_"
      },
      "source": [
        "cosine_loss = tf.keras.losses.CosineSimilarity(axis=1)\n",
        "def loss_function(actual, output):\n",
        "    loss = 1-cosine_loss(actual, output)\n",
        "    return loss\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpmOmRqGpWKV"
      },
      "source": [
        "opt = Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
        "seq2seq_model.compile(optimizer=opt, loss = loss_function)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wm0p1KLC0q0h"
      },
      "source": [
        "encoder_input = np.zeros([count,sequence_length,300])\n",
        "decoder_input = np.zeros([count,sequence_length,300])\n",
        "decoder_target = np.zeros([count,sequence_length,300])\n",
        "for i, (data1, data2) in enumerate(zip(data_set['clean_message'][0:count],data_set['clean_reply'][0:count])):\n",
        "    seq1 = [vocab[word] for word in reversed(data1.split())]#from paper http://arxiv.org/abs/1409.3215v3\n",
        "    #havent understood about the <EOF> and the vector to represent that so we will think that '' is the EOF for now \n",
        "    seq2 = [vocab[words] for words in data1.split()]\n",
        "    for t in range(sequence_length) :\n",
        "        if t < sequence_length-len(seq1)-1 or t==sequence_length-1:\n",
        "            encoder_input[i,t] = np.array(vectors[0])\n",
        "        else :\n",
        "            encoder_input[i,t] = np.array(vectors[seq1[t-(sequence_length-len(seq1))]])\n",
        "        decoder_input[i,0] =  np.array(vectors[0])\n",
        "        if t < len(seq2):\n",
        "            decoder_target[i,t] = np.array(vectors[seq2[t]])\n",
        "        else :\n",
        "            decoder_target[i,t] = np.array(vectors[0])\n",
        "decoder_input[:,1:sequence_length] = decoder_target[:,0:sequence_length-1]      \n",
        "    # for words in data2.split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpQh9QJMHJGF"
      },
      "source": [
        "print(encoder_input[0])\n",
        "print(decoder_input[0])\n",
        "print(decoder_target[0])\n",
        "encoder_input.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQb8_7BVD74R"
      },
      "source": [
        "seq2seq_model.fit([encoder_input,decoder_input],decoder_target,batch_size=16,epochs=epochs,validation_split=.1)\n",
        "print(x.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4ka2lftG2Ix"
      },
      "source": [
        "import re\n",
        "def remove(message):\n",
        "    pure = ''\n",
        "    for letter in message:\n",
        "        if letter in 'abcdefghijklmnopqrstuvwxyz' or letter == ' ':\n",
        "              pure = pure + letter\n",
        "    # print(pure)\n",
        "    return pure\n",
        "        \n",
        "def remove_haha(message):\n",
        "\n",
        "    if re.search('haha',message):\n",
        "            word = 'haha'\n",
        "    else :\n",
        "            word = message\n",
        "    return word\n",
        "def vocab_change(message):\n",
        "    token = []\n",
        "    for words in message:\n",
        "        if words in vocab.keys():\n",
        "            token.append(vocab[words])\n",
        "        else :\n",
        "            token.append(1)\n",
        "    return token \n",
        "def prediction_pattern(message):\n",
        "    message = remove(message)\n",
        "    tokens = [remove_haha(word) for word in reversed(message.split())]\n",
        "    seq1 = vocab_change(tokens)\n",
        "    encoder_inp = np.zeros([1,30,vector_shape])\n",
        "    for t in range(sequence_length) :\n",
        "        if t < sequence_length-len(seq1)-1 or t==sequence_length-1:\n",
        "            encoder_inp[0,t] = vectors[0]\n",
        "        else :\n",
        "            encoder_inp[0,t] = vectors[seq1[t-(sequence_length-len(seq1))]]\n",
        "    return encoder_inp\n",
        "prediction_pattern(('oe k gardai xas'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6ATfd7aVz8V"
      },
      "source": [
        "encoder_model =Model(encoder_input,encoder_states) #thans for https://medium.com/deep-learning-with-keras/seq2seq-part-d-encoder-decoder-with-teacher-forcing-18a3a09a096\n",
        "#took some time to understand the everything but well written and well implemented\n",
        "decoder_state_input_h = Input(shape=(vector_shape,))\n",
        "decoder_state_input_c = Input(shape=(vector_shape,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DNaSRlUl_Y-"
      },
      "source": [
        "def decode(sentences):\n",
        "    encoder_input = prediction_pattern(sentences.lower())\n",
        "    states_value = encoder_model.predict(encoder_input)\n",
        "    target_seq = np.zeros([1,1,vector_shape])\n",
        "    target_seq[0,0,vector_shape] = vectors[0]\n",
        "    print(target_seq)\n",
        "    stop_condition = True \n",
        "    while not stop_condition:\n",
        "\n",
        "        # in a loop\n",
        "        # decode the input to a token/output prediction + required states for context vector\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "\n",
        "        # convert the token/output prediction to a token/output\n",
        "        sampled_token_index = np.argmax(vectors*np.squeeze(output_tokens))\n",
        "        print(vocab_reverse[sampled_token_index])\n",
        "        # add the predicted token/output to output sequence\n",
        "        \n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        decoded_seq.append(vocab_reverse[sampled_token_index)\n",
        "        if (sampled_token_index == 0 or\n",
        "           len(decoded_seq) == n_timesteps_in):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the input target sequence (of length 1) \n",
        "        # with the predicted token/output \n",
        "        target_seq = np.zeros((1, 1, n_features))\n",
        "        target_seq[0, 0, sampled_token_index] = vectors['sampled_token_index']\n",
        "\n",
        "        # Update input states (context vector) \n",
        "        # with the ouputed states\n",
        "        states_value = [h, c]\n",
        "\n",
        "        # loop back.....\n",
        "        \n",
        "    # when loop exists return the output sequence\n",
        "    return decoded_seq\n",
        "\n",
        "\n",
        "decode('oe k gardai xas')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58JvLo1r34OP"
      },
      "source": [
        "target_seq = np.zeros([1,1,vector_shape])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22OXzU9dV2LD"
      },
      "source": [
        "print(vectors.shape)\n",
        "target = target_seq.reshape([300])\n",
        "np.dot(vectors,target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RofmdDdxV4_O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}