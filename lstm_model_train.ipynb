{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm_model_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1jdzNaMDwFBd-YZ1d5LBMp4WL2eFKGpkU",
      "authorship_tag": "ABX9TyP6+8ai4grvIEOJKEtcuFVe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sameepshrestha/Mission-Chatbot/blob/main/lstm_model_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oK872g-oMSg4"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import LSTM\n",
        "import csv \n",
        "import pandas as pd \n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.activations import  sigmoid, tanh\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np \n",
        "# from sklearn.preprocessing import train_test_split\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RtgWiym7Zhc"
      },
      "source": [
        "# encoder and decoder both input_shape = [batch ,time_step,feature] \n",
        "batch_size = 64\n",
        "time_step = 31\n",
        "epochs = 200\n",
        "vector_shape  = 300  #the loss we wil be using is the cosine \n",
        "#the ct and the ht learned from the encoder as a context is passed to the decoder \n",
        "data_set = pd.read_csv('/content/drive/MyDrive/chat bots/trainable_dataset.csv')\n",
        "count = len(data_set)\n",
        "count=25000"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yNWn1LRJiPH"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9TLVpGr1qDa"
      },
      "source": [
        "\n",
        "vocab = {}\n",
        "vocab_reverse = {}\n",
        "with open ('/content/drive/MyDrive/chat bots/metadata (1).tsv') as f:\n",
        "    for i,row in enumerate(csv.reader(f )):\n",
        "        vocab[row[0]] = i\n",
        "        vocab_reverse[i] = row[0]\n",
        "total_size = len(vocab)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTctAe9i1_FB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c80cd26-bf97-4a50-b3b8-dba459ac03bd"
      },
      "source": [
        "\n",
        "vectors = np.zeros([total_size,vector_shape])\n",
        "with open ('/content/drive/MyDrive/chat bots/vectors (1).tsv') as f:\n",
        "    for i,row in enumerate(csv.reader(f,delimiter=\"\\t\")):\n",
        "        vectors[i,:] = [np.array(float(x)) for x in row]\n",
        "print(vectors.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(38534, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxZ-RZfa72Nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "511f3844-b255-49b9-d751-05d2c4147242"
      },
      "source": [
        "# well this is a mess no stacked as one opposes stack and other support it but i think the attention model \n",
        "#without the stack should also be the best option >next target if this becomes some what sucessful\n",
        "sequence_length =  30\n",
        "encoder_inputs = Input(shape=( sequence_length,vector_shape))\n",
        "encoder = LSTM(300, return_sequences=False, return_state=True)\n",
        "encoder_outputs, stateh, statec = encoder(encoder_inputs)\n",
        "encoder_states = [stateh,statec]\n",
        "decoder_inputs = Input(shape= (None,vector_shape))\n",
        "decoder = LSTM(300,return_sequences=True, return_state=True )\n",
        "decoder_outputs,_ ,_  = decoder(decoder_inputs, initial_state = encoder_states)\n",
        "\n",
        "decoder_dense = Dense(vector_shape, activation = tanh)\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "seq2seq_model = Model([encoder_inputs,decoder_inputs],decoder_outputs)\n",
        "seq2seq_model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 30, 300)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None, 300)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 300), (None, 721200      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, None, 300),  721200      input_2[0][0]                    \n",
            "                                                                 lstm[0][1]                       \n",
            "                                                                 lstm[0][2]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 300)    90300       lstm_1[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 1,532,700\n",
            "Trainable params: 1,532,700\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFCeQS2-369_"
      },
      "source": [
        "cosine_loss = tf.keras.losses.CosineSimilarity(axis=1)\n",
        "def loss_function(actual, output):\n",
        "    loss = 1-cosine_loss(actual, output)\n",
        "    return loss\n",
        "    "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpmOmRqGpWKV"
      },
      "source": [
        "opt = Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
        "seq2seq_model.compile(optimizer=opt, loss = loss_function)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wm0p1KLC0q0h"
      },
      "source": [
        "encoder_input = np.zeros([count,sequence_length,300])\n",
        "decoder_input = np.zeros([count,sequence_length,300])\n",
        "decoder_target = np.zeros([count,sequence_length,300])\n",
        "for i, (data1, data2) in enumerate(zip(data_set['clean_message'][0:count],data_set['clean_reply'][0:count])):\n",
        "    seq1 = [vocab[word] for word in reversed(data1.split())]#from paper http://arxiv.org/abs/1409.3215v3\n",
        "    #havent understood about the <EOF> and the vector to represent that so we will think that '' is the EOF for now \n",
        "    seq2 = [vocab[words] for words in data1.split()]\n",
        "    for t in range(sequence_length) :\n",
        "        if t < sequence_length-len(seq1)-1 or t==sequence_length-1:\n",
        "            encoder_input[i,t] = np.array(vectors[0])\n",
        "        else :\n",
        "            encoder_input[i,t] = np.array(vectors[seq1[t-(sequence_length-len(seq1))]])\n",
        "        decoder_input[i,0] =  np.array(vectors[0])\n",
        "        if t < len(seq2):\n",
        "            decoder_target[i,t] = np.array(vectors[seq2[t]])\n",
        "        else :\n",
        "            decoder_target[i,t] = np.array(vectors[0])\n",
        "decoder_input[:,1:sequence_length] = decoder_target[:,0:sequence_length-1]      \n",
        "    # for words in data2.split()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpQh9QJMHJGF",
        "outputId": "3da2552b-78b8-4546-d714-58e4bbe18cc2"
      },
      "source": [
        "print(encoder_input[0])\n",
        "print(decoder_input[0])\n",
        "print(decoder_target[0])\n",
        "encoder_input.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.01289351  0.04199443 -0.04290298 ...  0.01144711 -0.01062251\n",
            "   0.03836933]\n",
            " [ 0.01289351  0.04199443 -0.04290298 ...  0.01144711 -0.01062251\n",
            "   0.03836933]\n",
            " [ 0.01289351  0.04199443 -0.04290298 ...  0.01144711 -0.01062251\n",
            "   0.03836933]\n",
            " ...\n",
            " [-0.15924105 -0.2504874   0.14590351 ...  0.2649294  -0.04659399\n",
            "  -0.20838113]\n",
            " [ 0.18101086 -0.14749381  0.32382107 ... -0.12810606  0.06554159\n",
            "  -0.05702667]\n",
            " [ 0.01289351  0.04199443 -0.04290298 ...  0.01144711 -0.01062251\n",
            "   0.03836933]]\n",
            "[[ 0.01289351  0.04199443 -0.04290298 ...  0.01144711 -0.01062251\n",
            "   0.03836933]\n",
            " [ 0.1761622   0.02412698 -0.25381556 ... -0.27347195  0.38541627\n",
            "   0.4362455 ]\n",
            " [ 0.18101086 -0.14749381  0.32382107 ... -0.12810606  0.06554159\n",
            "  -0.05702667]\n",
            " ...\n",
            " [ 0.01289351  0.04199443 -0.04290298 ...  0.01144711 -0.01062251\n",
            "   0.03836933]\n",
            " [ 0.01289351  0.04199443 -0.04290298 ...  0.01144711 -0.01062251\n",
            "   0.03836933]\n",
            " [ 0.01289351  0.04199443 -0.04290298 ...  0.01144711 -0.01062251\n",
            "   0.03836933]]\n",
            "[[ 0.1761622   0.02412698 -0.25381556 ... -0.27347195  0.38541627\n",
            "   0.4362455 ]\n",
            " [ 0.18101086 -0.14749381  0.32382107 ... -0.12810606  0.06554159\n",
            "  -0.05702667]\n",
            " [-0.15924105 -0.2504874   0.14590351 ...  0.2649294  -0.04659399\n",
            "  -0.20838113]\n",
            " ...\n",
            " [ 0.01289351  0.04199443 -0.04290298 ...  0.01144711 -0.01062251\n",
            "   0.03836933]\n",
            " [ 0.01289351  0.04199443 -0.04290298 ...  0.01144711 -0.01062251\n",
            "   0.03836933]\n",
            " [ 0.01289351  0.04199443 -0.04290298 ...  0.01144711 -0.01062251\n",
            "   0.03836933]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 30, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQb8_7BVD74R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ad39e9ba-7127-4817-ba01-447c043c2568"
      },
      "source": [
        "seq2seq_model.fit([encoder_input,decoder_input],decoder_target,batch_size=16,epochs=epochs,validation_split=.1)\n",
        "print(x.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1407/1407 [==============================] - 69s 26ms/step - loss: 0.7172 - val_loss: 0.6132\n",
            "Epoch 2/200\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 0.5327 - val_loss: 0.5355\n",
            "Epoch 3/200\n",
            "1407/1407 [==============================] - 36s 25ms/step - loss: 0.4602 - val_loss: 0.4941\n",
            "Epoch 4/200\n",
            "1407/1407 [==============================] - 36s 25ms/step - loss: 0.4216 - val_loss: 0.4658\n",
            "Epoch 5/200\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 0.3930 - val_loss: 0.4465\n",
            "Epoch 6/200\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 0.3720 - val_loss: 0.4298\n",
            "Epoch 7/200\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 0.3571 - val_loss: 0.4150\n",
            "Epoch 8/200\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 0.3424 - val_loss: 0.4048\n",
            "Epoch 9/200\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 0.3316 - val_loss: 0.3936\n",
            "Epoch 10/200\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 0.3205 - val_loss: 0.3835\n",
            "Epoch 11/200\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 0.3128 - val_loss: 0.3755\n",
            "Epoch 12/200\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 0.3039 - val_loss: 0.3687\n",
            "Epoch 13/200\n",
            "1407/1407 [==============================] - 36s 25ms/step - loss: 0.2971 - val_loss: 0.3600\n",
            "Epoch 14/200\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 0.2883 - val_loss: 0.3554\n",
            "Epoch 15/200\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 0.2831 - val_loss: 0.3458\n",
            "Epoch 16/200\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 0.2766 - val_loss: 0.3424\n",
            "Epoch 17/200\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 0.2717 - val_loss: 0.3342\n",
            "Epoch 18/200\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 0.2653 - val_loss: 0.3302\n",
            "Epoch 19/200\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 0.2593 - val_loss: 0.3237\n",
            "Epoch 20/200\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 0.2569 - val_loss: 0.3219\n",
            "Epoch 21/200\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 0.2532 - val_loss: 0.3179\n",
            "Epoch 22/200\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 0.2474 - val_loss: 0.3117\n",
            "Epoch 23/200\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 0.2450 - val_loss: 0.3088\n",
            "Epoch 24/200\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 0.2404 - val_loss: 0.3056\n",
            "Epoch 25/200\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 0.2358 - val_loss: 0.3007\n",
            "Epoch 26/200\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 0.2320 - val_loss: 0.2991\n",
            "Epoch 27/200\n",
            "1407/1407 [==============================] - 36s 25ms/step - loss: 0.2301 - val_loss: 0.2954\n",
            "Epoch 28/200\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 0.2271 - val_loss: 0.2920\n",
            "Epoch 29/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.2242 - val_loss: 0.2895\n",
            "Epoch 30/200\n",
            "1407/1407 [==============================] - 36s 25ms/step - loss: 0.2216 - val_loss: 0.2909\n",
            "Epoch 31/200\n",
            "1407/1407 [==============================] - 36s 25ms/step - loss: 0.2195 - val_loss: 0.2851\n",
            "Epoch 32/200\n",
            "1407/1407 [==============================] - 36s 25ms/step - loss: 0.2161 - val_loss: 0.2827\n",
            "Epoch 33/200\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 0.2149 - val_loss: 0.2806\n",
            "Epoch 34/200\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 0.2126 - val_loss: 0.2788\n",
            "Epoch 35/200\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 0.2119 - val_loss: 0.2760\n",
            "Epoch 36/200\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 0.2080 - val_loss: 0.2737\n",
            "Epoch 37/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.2071 - val_loss: 0.2714\n",
            "Epoch 38/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.2033 - val_loss: 0.2690\n",
            "Epoch 39/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.2012 - val_loss: 0.2692\n",
            "Epoch 40/200\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.2019 - val_loss: 0.2675\n",
            "Epoch 41/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1991 - val_loss: 0.2650\n",
            "Epoch 42/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1973 - val_loss: 0.2647\n",
            "Epoch 43/200\n",
            "1407/1407 [==============================] - 36s 25ms/step - loss: 0.1970 - val_loss: 0.2634\n",
            "Epoch 44/200\n",
            "1407/1407 [==============================] - 36s 25ms/step - loss: 0.1964 - val_loss: 0.2610\n",
            "Epoch 45/200\n",
            "1407/1407 [==============================] - 36s 25ms/step - loss: 0.1943 - val_loss: 0.2580\n",
            "Epoch 46/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1922 - val_loss: 0.2572\n",
            "Epoch 47/200\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.1909 - val_loss: 0.2563\n",
            "Epoch 48/200\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.1909 - val_loss: 0.2548\n",
            "Epoch 49/200\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.1899 - val_loss: 0.2526\n",
            "Epoch 50/200\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.1881 - val_loss: 0.2515\n",
            "Epoch 51/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1850 - val_loss: 0.2501\n",
            "Epoch 52/200\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.1844 - val_loss: 0.2495\n",
            "Epoch 53/200\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.1844 - val_loss: 0.2492\n",
            "Epoch 54/200\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.1834 - val_loss: 0.2481\n",
            "Epoch 55/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1816 - val_loss: 0.2468\n",
            "Epoch 56/200\n",
            "1407/1407 [==============================] - 36s 25ms/step - loss: 0.1798 - val_loss: 0.2458\n",
            "Epoch 57/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1804 - val_loss: 0.2447\n",
            "Epoch 58/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1784 - val_loss: 0.2422\n",
            "Epoch 59/200\n",
            "1407/1407 [==============================] - 36s 25ms/step - loss: 0.1777 - val_loss: 0.2424\n",
            "Epoch 60/200\n",
            "1407/1407 [==============================] - 36s 25ms/step - loss: 0.1770 - val_loss: 0.2418\n",
            "Epoch 61/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1759 - val_loss: 0.2406\n",
            "Epoch 62/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1760 - val_loss: 0.2420\n",
            "Epoch 63/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1745 - val_loss: 0.2394\n",
            "Epoch 64/200\n",
            "1407/1407 [==============================] - 36s 25ms/step - loss: 0.1742 - val_loss: 0.2379\n",
            "Epoch 65/200\n",
            "1407/1407 [==============================] - 36s 25ms/step - loss: 0.1720 - val_loss: 0.2379\n",
            "Epoch 66/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1730 - val_loss: 0.2355\n",
            "Epoch 67/200\n",
            "1407/1407 [==============================] - 36s 25ms/step - loss: 0.1700 - val_loss: 0.2367\n",
            "Epoch 68/200\n",
            "1407/1407 [==============================] - 36s 25ms/step - loss: 0.1706 - val_loss: 0.2346\n",
            "Epoch 69/200\n",
            "1407/1407 [==============================] - 36s 25ms/step - loss: 0.1707 - val_loss: 0.2339\n",
            "Epoch 70/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1703 - val_loss: 0.2331\n",
            "Epoch 71/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1688 - val_loss: 0.2318\n",
            "Epoch 72/200\n",
            "1407/1407 [==============================] - 36s 25ms/step - loss: 0.1679 - val_loss: 0.2317\n",
            "Epoch 73/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1672 - val_loss: 0.2314\n",
            "Epoch 74/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1659 - val_loss: 0.2299\n",
            "Epoch 75/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1661 - val_loss: 0.2301\n",
            "Epoch 76/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1663 - val_loss: 0.2292\n",
            "Epoch 77/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1643 - val_loss: 0.2292\n",
            "Epoch 78/200\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.1638 - val_loss: 0.2273\n",
            "Epoch 79/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1643 - val_loss: 0.2260\n",
            "Epoch 80/200\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.1629 - val_loss: 0.2264\n",
            "Epoch 81/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1625 - val_loss: 0.2253\n",
            "Epoch 82/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1627 - val_loss: 0.2248\n",
            "Epoch 83/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1614 - val_loss: 0.2244\n",
            "Epoch 84/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1605 - val_loss: 0.2241\n",
            "Epoch 85/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1597 - val_loss: 0.2235\n",
            "Epoch 86/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1575 - val_loss: 0.2214\n",
            "Epoch 87/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1610 - val_loss: 0.2223\n",
            "Epoch 88/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1585 - val_loss: 0.2212\n",
            "Epoch 89/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1572 - val_loss: 0.2227\n",
            "Epoch 90/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1568 - val_loss: 0.2199\n",
            "Epoch 91/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1564 - val_loss: 0.2185\n",
            "Epoch 92/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1545 - val_loss: 0.2180\n",
            "Epoch 93/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1555 - val_loss: 0.2172\n",
            "Epoch 94/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1544 - val_loss: 0.2181\n",
            "Epoch 95/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1539 - val_loss: 0.2193\n",
            "Epoch 96/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1530 - val_loss: 0.2166\n",
            "Epoch 97/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1534 - val_loss: 0.2172\n",
            "Epoch 98/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1522 - val_loss: 0.2164\n",
            "Epoch 99/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1531 - val_loss: 0.2153\n",
            "Epoch 100/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1513 - val_loss: 0.2157\n",
            "Epoch 101/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1518 - val_loss: 0.2156\n",
            "Epoch 102/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1503 - val_loss: 0.2134\n",
            "Epoch 103/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1505 - val_loss: 0.2136\n",
            "Epoch 104/200\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.1501 - val_loss: 0.2135\n",
            "Epoch 105/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1492 - val_loss: 0.2133\n",
            "Epoch 106/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1503 - val_loss: 0.2127\n",
            "Epoch 107/200\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.1504 - val_loss: 0.2129\n",
            "Epoch 108/200\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.1488 - val_loss: 0.2115\n",
            "Epoch 109/200\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.1477 - val_loss: 0.2111\n",
            "Epoch 110/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1475 - val_loss: 0.2109\n",
            "Epoch 111/200\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.1475 - val_loss: 0.2099\n",
            "Epoch 112/200\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.1464 - val_loss: 0.2096\n",
            "Epoch 113/200\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.1466 - val_loss: 0.2099\n",
            "Epoch 114/200\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.1462 - val_loss: 0.2081\n",
            "Epoch 115/200\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.1450 - val_loss: 0.2095\n",
            "Epoch 116/200\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.1454 - val_loss: 0.2072\n",
            "Epoch 117/200\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.1434 - val_loss: 0.2073\n",
            "Epoch 118/200\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.1432 - val_loss: 0.2089\n",
            "Epoch 119/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1446 - val_loss: 0.2071\n",
            "Epoch 120/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1430 - val_loss: 0.2052\n",
            "Epoch 121/200\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.1440 - val_loss: 0.2071\n",
            "Epoch 122/200\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.1427 - val_loss: 0.2061\n",
            "Epoch 123/200\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.1437 - val_loss: 0.2068\n",
            "Epoch 124/200\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.1423 - val_loss: 0.2069\n",
            "Epoch 125/200\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.1425 - val_loss: 0.2064\n",
            "Epoch 126/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1406 - val_loss: 0.2054\n",
            "Epoch 127/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1425 - val_loss: 0.2028\n",
            "Epoch 128/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1409 - val_loss: 0.2028\n",
            "Epoch 129/200\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.1405 - val_loss: 0.2023\n",
            "Epoch 130/200\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.1414 - val_loss: 0.2029\n",
            "Epoch 131/200\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.1409 - val_loss: 0.2040\n",
            "Epoch 132/200\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.1402 - val_loss: 0.2023\n",
            "Epoch 133/200\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.1390 - val_loss: 0.2041\n",
            "Epoch 134/200\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.1386 - val_loss: 0.2013\n",
            "Epoch 135/200\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.1403 - val_loss: 0.2015\n",
            "Epoch 136/200\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.1381 - val_loss: 0.2021\n",
            "Epoch 137/200\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.1389 - val_loss: 0.2007\n",
            "Epoch 138/200\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.1372 - val_loss: 0.1999\n",
            "Epoch 139/200\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.1376 - val_loss: 0.1996\n",
            "Epoch 140/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1378 - val_loss: 0.2002\n",
            "Epoch 141/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1380 - val_loss: 0.1995\n",
            "Epoch 142/200\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.1378 - val_loss: 0.1987\n",
            "Epoch 143/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1372 - val_loss: 0.1989\n",
            "Epoch 144/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1367 - val_loss: 0.1977\n",
            "Epoch 145/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1365 - val_loss: 0.1975\n",
            "Epoch 146/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1369 - val_loss: 0.1977\n",
            "Epoch 147/200\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.1347 - val_loss: 0.1966\n",
            "Epoch 148/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1354 - val_loss: 0.1966\n",
            "Epoch 149/200\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.1362 - val_loss: 0.1978\n",
            "Epoch 150/200\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.1365 - val_loss: 0.1960\n",
            "Epoch 151/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1350 - val_loss: 0.1961\n",
            "Epoch 152/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1340 - val_loss: 0.1968\n",
            "Epoch 153/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1343 - val_loss: 0.1957\n",
            "Epoch 154/200\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.1329 - val_loss: 0.1953\n",
            "Epoch 155/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1324 - val_loss: 0.1950\n",
            "Epoch 156/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1334 - val_loss: 0.1949\n",
            "Epoch 157/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1327 - val_loss: 0.1941\n",
            "Epoch 158/200\n",
            "1407/1407 [==============================] - 36s 25ms/step - loss: 0.1323 - val_loss: 0.1954\n",
            "Epoch 159/200\n",
            "1407/1407 [==============================] - 36s 25ms/step - loss: 0.1322 - val_loss: 0.1963\n",
            "Epoch 160/200\n",
            "1407/1407 [==============================] - 36s 25ms/step - loss: 0.1310 - val_loss: 0.1953\n",
            "Epoch 161/200\n",
            "1407/1407 [==============================] - 36s 25ms/step - loss: 0.1322 - val_loss: 0.1936\n",
            "Epoch 162/200\n",
            "1407/1407 [==============================] - 36s 25ms/step - loss: 0.1323 - val_loss: 0.1942\n",
            "Epoch 163/200\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 0.1312 - val_loss: 0.1929\n",
            "Epoch 164/200\n",
            "1407/1407 [==============================] - 36s 25ms/step - loss: 0.1311 - val_loss: 0.1945\n",
            "Epoch 165/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1318 - val_loss: 0.1925\n",
            "Epoch 166/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1318 - val_loss: 0.1918\n",
            "Epoch 167/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1312 - val_loss: 0.1926\n",
            "Epoch 168/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1312 - val_loss: 0.1939\n",
            "Epoch 169/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1299 - val_loss: 0.1912\n",
            "Epoch 170/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1302 - val_loss: 0.1915\n",
            "Epoch 171/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1307 - val_loss: 0.1909\n",
            "Epoch 172/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1301 - val_loss: 0.1909\n",
            "Epoch 173/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1288 - val_loss: 0.1905\n",
            "Epoch 174/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1294 - val_loss: 0.1916\n",
            "Epoch 175/200\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.1303 - val_loss: 0.1902\n",
            "Epoch 176/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1289 - val_loss: 0.1892\n",
            "Epoch 177/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1285 - val_loss: 0.1893\n",
            "Epoch 178/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1297 - val_loss: 0.1906\n",
            "Epoch 179/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1279 - val_loss: 0.1897\n",
            "Epoch 180/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1297 - val_loss: 0.1931\n",
            "Epoch 181/200\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.1279 - val_loss: 0.1896\n",
            "Epoch 182/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1290 - val_loss: 0.1887\n",
            "Epoch 183/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1278 - val_loss: 0.1905\n",
            "Epoch 184/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1280 - val_loss: 0.1896\n",
            "Epoch 185/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1273 - val_loss: 0.1891\n",
            "Epoch 186/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1273 - val_loss: 0.1881\n",
            "Epoch 187/200\n",
            "1407/1407 [==============================] - 36s 25ms/step - loss: 0.1262 - val_loss: 0.1891\n",
            "Epoch 188/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1263 - val_loss: 0.1875\n",
            "Epoch 189/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1258 - val_loss: 0.1883\n",
            "Epoch 190/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1270 - val_loss: 0.1881\n",
            "Epoch 191/200\n",
            "1407/1407 [==============================] - 36s 25ms/step - loss: 0.1271 - val_loss: 0.1884\n",
            "Epoch 192/200\n",
            "1407/1407 [==============================] - 36s 25ms/step - loss: 0.1266 - val_loss: 0.1877\n",
            "Epoch 193/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1269 - val_loss: 0.1862\n",
            "Epoch 194/200\n",
            "1407/1407 [==============================] - 36s 25ms/step - loss: 0.1262 - val_loss: 0.1868\n",
            "Epoch 195/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1264 - val_loss: 0.1861\n",
            "Epoch 196/200\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.1251 - val_loss: 0.1868\n",
            "Epoch 197/200\n",
            "1407/1407 [==============================] - 36s 25ms/step - loss: 0.1252 - val_loss: 0.1858\n",
            "Epoch 198/200\n",
            "1407/1407 [==============================] - 36s 25ms/step - loss: 0.1246 - val_loss: 0.1858\n",
            "Epoch 199/200\n",
            "1407/1407 [==============================] - 36s 25ms/step - loss: 0.1248 - val_loss: 0.1854\n",
            "Epoch 200/200\n",
            "1407/1407 [==============================] - 36s 25ms/step - loss: 0.1249 - val_loss: 0.1866\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-3a9af0238636>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mseq2seq_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoder_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder_target\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4ka2lftG2Ix",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9833da86-4274-4020-a76e-ee8a95f25bcb"
      },
      "source": [
        "import re\n",
        "def remove(message):\n",
        "    pure = ''\n",
        "    for letter in message:\n",
        "        if letter in 'abcdefghijklmnopqrstuvwxyz' or letter == ' ':\n",
        "              pure = pure + letter\n",
        "    # print(pure)\n",
        "    return pure\n",
        "        \n",
        "def remove_haha(message):\n",
        "\n",
        "    if re.search('haha',message):\n",
        "            word = 'haha'\n",
        "    else :\n",
        "            word = message\n",
        "    return word\n",
        "def vocab_change(message):\n",
        "    token = []\n",
        "    for words in message:\n",
        "        if words in vocab.keys():\n",
        "            token.append(vocab[words])\n",
        "        else :\n",
        "            token.append(1)\n",
        "    return token \n",
        "def prediction_pattern(message):\n",
        "    message = remove(message)\n",
        "    tokens = [remove_haha(word) for word in reversed(message.split())]\n",
        "    seq1 = vocab_change(tokens)\n",
        "    encoder_inp = np.zeros([1,30,vector_shape])\n",
        "    for t in range(sequence_length) :\n",
        "        if t < sequence_length-len(seq1)-1 or t==sequence_length-1:\n",
        "            encoder_inp[0,t] = vectors[0]\n",
        "        else :\n",
        "            encoder_inp[0,t] = vectors[seq1[t-(sequence_length-len(seq1))]]\n",
        "    return encoder_inp\n",
        "prediction_pattern(('oe k gardai xas'))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[ 0.01289351,  0.04199443, -0.04290298, ...,  0.01144711,\n",
              "         -0.01062251,  0.03836933],\n",
              "        [ 0.01289351,  0.04199443, -0.04290298, ...,  0.01144711,\n",
              "         -0.01062251,  0.03836933],\n",
              "        [ 0.01289351,  0.04199443, -0.04290298, ...,  0.01144711,\n",
              "         -0.01062251,  0.03836933],\n",
              "        ...,\n",
              "        [-0.27575025, -0.35908464,  0.2888842 , ...,  0.10760035,\n",
              "         -0.16621907, -0.13027316],\n",
              "        [-0.15924105, -0.2504874 ,  0.14590351, ...,  0.2649294 ,\n",
              "         -0.04659399, -0.20838113],\n",
              "        [ 0.01289351,  0.04199443, -0.04290298, ...,  0.01144711,\n",
              "         -0.01062251,  0.03836933]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6ATfd7aVz8V"
      },
      "source": [
        "encoder_model =Model(encoder_inputs,encoder_states) #thans for https://medium.com/deep-learning-with-keras/seq2seq-part-d-encoder-decoder-with-teacher-forcing-18a3a09a096\n",
        "#took some time to understand the everything but well written and well implemented\n",
        "decoder_state_input_h = Input(shape=(vector_shape,))\n",
        "decoder_state_input_c = Input(shape=(vector_shape,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_outputs, state_h, state_c = decoder(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DNaSRlUl_Y-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4208e22-f6b8-421e-dc34-e286ab8caa32"
      },
      "source": [
        "def decode(sentences):\n",
        "    n_timesteps_in = 30\n",
        "    encoder_input = prediction_pattern(sentences.lower())\n",
        "    states_value = encoder_model.predict(encoder_input)\n",
        "    target_seq = np.zeros([1,1,vector_shape])\n",
        "    target_seq[0,0,:] = vectors[0]\n",
        "    stop_condition = False \n",
        "    decoded_seq = []\n",
        "    while not stop_condition:\n",
        "\n",
        "        # in a loop\n",
        "        # decode the input to a token/output prediction + required states for context vector\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "        print(output_tokens.shape)\n",
        "        # convert the token/output prediction to a token/output\n",
        "        print((vectors*np.squeeze(output_tokens)).shape)\n",
        "        sampled_token_index = np.argmax(np.dot(vectors,np.squeeze(output_tokens)))\n",
        "        print(sampled_token_index)\n",
        "        print(vocab_reverse[sampled_token_index])\n",
        "        # add the predicted token/output to output sequence\n",
        "        \n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        decoded_seq.append(vocab_reverse[sampled_token_index])\n",
        "        if (sampled_token_index == 0 or\n",
        "           len(decoded_seq) == n_timesteps_in):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the input target sequence (of length 1) \n",
        "        # with the predicted token/output \n",
        "        target_seq = np.zeros((1, 1, vector_shape))\n",
        "        target_seq[0, 0, :] = vectors[sampled_token_index]\n",
        "\n",
        "        # Update input states (context vector) \n",
        "        # with the ouputed states\n",
        "        states_value = [h, c]\n",
        "\n",
        "        # loop back.....\n",
        "        \n",
        "    # when loop exists return the output sequence\n",
        "    return decoded_seq\n",
        "\n",
        "\n",
        "decode('oe k gardai xas')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 1, 300)\n",
            "(38534, 300)\n",
            "6332\n",
            "jhaan\n",
            "(1, 1, 300)\n",
            "(38534, 300)\n",
            "6332\n",
            "jhaan\n",
            "(1, 1, 300)\n",
            "(38534, 300)\n",
            "6332\n",
            "jhaan\n",
            "(1, 1, 300)\n",
            "(38534, 300)\n",
            "6332\n",
            "jhaan\n",
            "(1, 1, 300)\n",
            "(38534, 300)\n",
            "6332\n",
            "jhaan\n",
            "(1, 1, 300)\n",
            "(38534, 300)\n",
            "6332\n",
            "jhaan\n",
            "(1, 1, 300)\n",
            "(38534, 300)\n",
            "6332\n",
            "jhaan\n",
            "(1, 1, 300)\n",
            "(38534, 300)\n",
            "6332\n",
            "jhaan\n",
            "(1, 1, 300)\n",
            "(38534, 300)\n",
            "6332\n",
            "jhaan\n",
            "(1, 1, 300)\n",
            "(38534, 300)\n",
            "6332\n",
            "jhaan\n",
            "(1, 1, 300)\n",
            "(38534, 300)\n",
            "6332\n",
            "jhaan\n",
            "(1, 1, 300)\n",
            "(38534, 300)\n",
            "6332\n",
            "jhaan\n",
            "(1, 1, 300)\n",
            "(38534, 300)\n",
            "6332\n",
            "jhaan\n",
            "(1, 1, 300)\n",
            "(38534, 300)\n",
            "6332\n",
            "jhaan\n",
            "(1, 1, 300)\n",
            "(38534, 300)\n",
            "6332\n",
            "jhaan\n",
            "(1, 1, 300)\n",
            "(38534, 300)\n",
            "6332\n",
            "jhaan\n",
            "(1, 1, 300)\n",
            "(38534, 300)\n",
            "6332\n",
            "jhaan\n",
            "(1, 1, 300)\n",
            "(38534, 300)\n",
            "6332\n",
            "jhaan\n",
            "(1, 1, 300)\n",
            "(38534, 300)\n",
            "6332\n",
            "jhaan\n",
            "(1, 1, 300)\n",
            "(38534, 300)\n",
            "6332\n",
            "jhaan\n",
            "(1, 1, 300)\n",
            "(38534, 300)\n",
            "6332\n",
            "jhaan\n",
            "(1, 1, 300)\n",
            "(38534, 300)\n",
            "6332\n",
            "jhaan\n",
            "(1, 1, 300)\n",
            "(38534, 300)\n",
            "6332\n",
            "jhaan\n",
            "(1, 1, 300)\n",
            "(38534, 300)\n",
            "6332\n",
            "jhaan\n",
            "(1, 1, 300)\n",
            "(38534, 300)\n",
            "6332\n",
            "jhaan\n",
            "(1, 1, 300)\n",
            "(38534, 300)\n",
            "6332\n",
            "jhaan\n",
            "(1, 1, 300)\n",
            "(38534, 300)\n",
            "6332\n",
            "jhaan\n",
            "(1, 1, 300)\n",
            "(38534, 300)\n",
            "6332\n",
            "jhaan\n",
            "(1, 1, 300)\n",
            "(38534, 300)\n",
            "6332\n",
            "jhaan\n",
            "(1, 1, 300)\n",
            "(38534, 300)\n",
            "6332\n",
            "jhaan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['jhaan',\n",
              " 'jhaan',\n",
              " 'jhaan',\n",
              " 'jhaan',\n",
              " 'jhaan',\n",
              " 'jhaan',\n",
              " 'jhaan',\n",
              " 'jhaan',\n",
              " 'jhaan',\n",
              " 'jhaan',\n",
              " 'jhaan',\n",
              " 'jhaan',\n",
              " 'jhaan',\n",
              " 'jhaan',\n",
              " 'jhaan',\n",
              " 'jhaan',\n",
              " 'jhaan',\n",
              " 'jhaan',\n",
              " 'jhaan',\n",
              " 'jhaan',\n",
              " 'jhaan',\n",
              " 'jhaan',\n",
              " 'jhaan',\n",
              " 'jhaan',\n",
              " 'jhaan',\n",
              " 'jhaan',\n",
              " 'jhaan',\n",
              " 'jhaan',\n",
              " 'jhaan',\n",
              " 'jhaan']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58JvLo1r34OP"
      },
      "source": [
        "target_seq = np.zeros([1,1,vector_shape])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22OXzU9dV2LD"
      },
      "source": [
        "print(vectors.shape)\n",
        "target = target_seq.reshape([300])\n",
        "np.dot(vectors,target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RofmdDdxV4_O"
      },
      "source": [
        "gbvfhgfhgfhnfhjgfjhffghdgh"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}